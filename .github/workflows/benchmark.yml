name: Performance tracking
on:
  pull_request:

env:
  PYTHON: ~

jobs:
  performance-tracking:
    runs-on: ubuntu-latest
    #runs-on: self-hosted
    steps:
      # setup
      - uses: actions/checkout@v6
      - uses: julia-actions/setup-julia@latest
        with:
          version: '1.10'
      - uses: julia-actions/julia-buildpkg@latest
      - name: install dependencies
        run: julia -e 'using Pkg; pkg"add PkgBenchmark BenchmarkCI@0.1"'

      # run the benchmark suite
      - name: run benchmarks
        run: |
          julia -e '
            using BenchmarkCI
            try
              BenchmarkCI.judge(baseline = "origin/main")
              BenchmarkCI.displayjudgement()
            catch err
              workspace = BenchmarkCI.DEFAULT_WORKSPACE
              mkpath(workspace)
              target = joinpath(workspace, "result-target.json")
              baseline = joinpath(workspace, "result-baseline.json")
              open(joinpath(workspace, "baseline-error.txt"), "w") do io
                showerror(io, err, catch_backtrace())
              end
              if isfile(target) && !isfile(baseline)
                @warn "BenchmarkCI baseline failed; proceeding without comparison" exception=(err, catch_backtrace())
              else
                rethrow()
              end
            end
            '

      # generate and record the benchmark result as markdown
      - name: generate benchmark result
        run: |
          body=$(julia -e '
          using BenchmarkCI
          using PkgBenchmark

          let
              title = "Benchmark Result"
              workspace = BenchmarkCI.DEFAULT_WORKSPACE
              target = joinpath(workspace, "result-target.json")
              baseline = joinpath(workspace, "result-baseline.json")
              baseline_error = joinpath(workspace, "baseline-error.txt")
              if isfile(target) && isfile(baseline)
                  judgement = BenchmarkCI._loadjudge(workspace)
                  ciresult = BenchmarkCI.CIResult(; judgement, title)
                  BenchmarkCI.printcommentmd(stdout::IO, ciresult)
              else
                  println("## $title")
                  println()
                  println("Baseline benchmarks could not be run for comparison.")
                  if isfile(baseline_error)
                      println()
                      println("<details><summary>baseline error</summary>")
                      println()
                      println("```")
                      print(read(baseline_error, String))
                      println()
                      println("```")
                      println()
                      println("</details>")
                  end
              end
          end
          ')
          body="${body//'%'/'%25'}"
          body="${body//$'\n'/'%0A'}"
          body="${body//$'\r'/'%0D'}"
          echo $body > ./benchmark-result.artifact

      # record the pull request number
      - name: record pull request number
        run: echo ${{ github.event.pull_request.number }} > ./pull-request-number.artifact

      # save as artifacts (performance tracking (comment) workflow will use it)
      - uses: actions/upload-artifact@v5
        with:
          name: performance-tracking
          path: ./*.artifact
